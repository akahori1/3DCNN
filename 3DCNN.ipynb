{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet_3D_3(nn.Module):\n",
    "    '''Resnet_3D_3'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Resnet_3D_3, self).__init__()\n",
    "        \n",
    "        self.res3a_2 = nn.Conv3d(96, 128, kernel_size=(\n",
    "            3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        \n",
    "        self.res3a_bn = nn.BatchNorm3d(\n",
    "            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.res3a_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.res3b_1 = nn.Conv3d(128, 128, kernel_size=(\n",
    "            3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        self.res3b_1_bn = nn.BatchNorm3d(\n",
    "            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.res3b_1_relu = nn.ReLU(inplace=True)\n",
    "        self.res3b_2 = nn.Conv3d(128, 128, kernel_size=(\n",
    "            3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        \n",
    "        self.res3b_bn = nn.BatchNorm3d(\n",
    "            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.res3b_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = self.res3a_2(x)\n",
    "        out = self.res3a_bn(residual)\n",
    "        out = self.res3a_relu(out)\n",
    "\n",
    "        out = self.res3b_1(out)\n",
    "        out = self.res3b_1_bn(out)\n",
    "        out = self.res3b_relu(out)\n",
    "        out = self.res3b_2(out)\n",
    "\n",
    "        out += residual\n",
    "\n",
    "        out = self.res3b_bn(out)\n",
    "        out = self.res3b_relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet_3D_4(nn.Module):\n",
    "    '''Resnet_3D_4'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Resnet_3D_4, self).__init__()\n",
    "\n",
    "        self.res4a_1 = nn.Conv3d(128, 256, kernel_size=(\n",
    "            3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
    "        self.res4a_1_bn = nn.BatchNorm3d(\n",
    "            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.res4a_1_relu = nn.ReLU(inplace=True)\n",
    "        self.res4a_2 = nn.Conv3d(256, 256, kernel_size=(\n",
    "            3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        \n",
    "        self.res4a_down = nn.Conv3d(128, 256, kernel_size=(\n",
    "            3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
    "        \n",
    "        self.res4a_bn = nn.BatchNorm3d(\n",
    "            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.res4a_relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.res4b_1 = nn.Conv3d(256, 256, kernel_size=(\n",
    "            3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        self.res4b_1_bn = nn.BatchNorm3d(\n",
    "            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.res4b_1_relu = nn.ReLU(inplace=True)\n",
    "        self.res4b_2 = nn.Conv3d(256, 256, kernel_size=(\n",
    "            3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        \n",
    "        self.res4b_bn = nn.BatchNorm3d(\n",
    "            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.res4b_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.res4a_down(x)\n",
    "\n",
    "        out = self.res4a_1(x)\n",
    "        out = self.res4a_1_bn(out)\n",
    "        out = self.res4a_1_relu(out)\n",
    "\n",
    "        out = self.res4a_2(out)\n",
    "\n",
    "        out += residual\n",
    "\n",
    "        residual2 = out\n",
    "\n",
    "        out = self.res4a_bn(out)\n",
    "        out = self.res4a_relu(out)\n",
    "\n",
    "        out = self.res4b_1(out)\n",
    "\n",
    "        out = self.res4b_1_bn(out)\n",
    "        out = self.res4b_1_relu(out)\n",
    "\n",
    "        out = self.res4b_2(out)\n",
    "\n",
    "        out += residual2\n",
    "\n",
    "        out = self.res4b_bn(out)\n",
    "        out = self.res4b_relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet_3D_5(nn.Module):\n",
    "    '''Resnet_3D_5'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Resnet_3D_5, self).__init__()\n",
    "        \n",
    "        self.res5a_1 = nn.Conv3d(256, 512, kernel_size=(\n",
    "            3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
    "        self.res5a_1_bn = nn.BatchNorm3d(\n",
    "            512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.res5a_1_relu = nn.ReLU(inplace=True)\n",
    "        self.res5a_2 = nn.Conv3d(512, 512, kernel_size=(\n",
    "            3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        \n",
    "        self.res5a_down = nn.Conv3d(256, 512, kernel_size=(\n",
    "            3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
    "        \n",
    "        self.res5a_bn = nn.BatchNorm3d(\n",
    "            512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.res5a_relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.res5b_1 = nn.Conv3d(512, 512, kernel_size=(\n",
    "            3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        self.res5b_1_bn = nn.BatchNorm3d(\n",
    "            512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.res5b_1_relu = nn.ReLU(inplace=True)\n",
    "        self.res5b_2 = nn.Conv3d(512, 512, kernel_size=(\n",
    "            3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        \n",
    "        self.res5b_bn = nn.BatchNorm3d(\n",
    "            512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.res5b_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.res5a_down(x)\n",
    "\n",
    "        out = self.res5a_1(x)\n",
    "        out = self.res5a_1_bn(out)\n",
    "        out = self.res5a_1_relu(out)\n",
    "\n",
    "        out = self.res5a_2(out)\n",
    "\n",
    "        out += residual  # res5a\n",
    "\n",
    "        residual2 = out\n",
    "\n",
    "        out = self.res5a_bn(out)\n",
    "        out = self.res5a_relu(out)\n",
    "\n",
    "        out = self.res5b_1(out)\n",
    "\n",
    "        out = self.res5b_1_bn(out)\n",
    "        out = self.res5b_1_relu(out)\n",
    "\n",
    "        out = self.res5b_2(out)\n",
    "\n",
    "        out += residual2  # res5b\n",
    "\n",
    "        out = self.res5b_bn(out)\n",
    "        out = self.res5b_relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECO_3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECO_3D, self).__init__()\n",
    "\n",
    "        # 3D_Resnetジュール\n",
    "        self.res_3d_3 = Resnet_3D_3()\n",
    "        self.res_3d_4 = Resnet_3D_4()\n",
    "        self.res_3d_5 = Resnet_3D_5()\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.global_pool = nn.AvgPool3d(\n",
    "            kernel_size=(4, 7, 7), stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        入力xのサイズtorch.Size([batch_num,frames, 96, 28, 28]))\n",
    "        '''\n",
    "        out = torch.transpose(x, 1, 2)  # テンソルの順番入れ替え\n",
    "        out = self.res_3d_3(out)\n",
    "        out = self.res_3d_4(out)\n",
    "        out = self.res_3d_5(out)\n",
    "        out = self.global_pool(out)\n",
    "        \n",
    "        # テンソルサイズを変更\n",
    "        # torch.Size([batch_num, 512, 1, 1, 1])からtorch.Size([batch_num, 512])へ\n",
    "        out =out.view(out.size()[0], out.size()[1])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ECO_3D(\n",
       "  (res_3d_3): Resnet_3D_3(\n",
       "    (res3a_2): Conv3d(96, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (res3a_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (res3a_relu): ReLU(inplace=True)\n",
       "    (res3b_1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (res3b_1_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (res3b_1_relu): ReLU(inplace=True)\n",
       "    (res3b_2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (res3b_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (res3b_relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (res_3d_4): Resnet_3D_4(\n",
       "    (res4a_1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (res4a_1_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (res4a_1_relu): ReLU(inplace=True)\n",
       "    (res4a_2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (res4a_down): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (res4a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (res4a_relu): ReLU(inplace=True)\n",
       "    (res4b_1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (res4b_1_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (res4b_1_relu): ReLU(inplace=True)\n",
       "    (res4b_2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (res4b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (res4b_relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (res_3d_5): Resnet_3D_5(\n",
       "    (res5a_1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (res5a_1_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (res5a_1_relu): ReLU(inplace=True)\n",
       "    (res5a_2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (res5a_down): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (res5a_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (res5a_relu): ReLU(inplace=True)\n",
       "    (res5b_1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (res5b_1_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (res5b_1_relu): ReLU(inplace=True)\n",
       "    (res5b_2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (res5b_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (res5b_relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (global_pool): AvgPool3d(kernel_size=(4, 7, 7), stride=1, padding=0)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#モデルの用意\n",
    "net = ECO_3D()\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 516.2 MB 2.5 kB/s  eta 0:00:01     |███████▏                        | 114.8 MB 55.0 MB/s eta 0:00:08     |██████████████▉                 | 239.6 MB 60.1 MB/s eta 0:00:05\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.34.2)\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 28 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 9.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 52.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.18.1)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.1-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 55.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 3.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.29.0-cp37-cp37m-manylinux2010_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 52.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.4.1)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 65.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.11.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 13.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)\n",
      "\u001b[K     |████████████████████████████████| 777 kB 51.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (46.1.3.post20200330)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.15.0-py2.py3-none-any.whl (89 kB)\n",
      "\u001b[K     |████████████████████████████████| 89 kB 15.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.5.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.1)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.4.2)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 66.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.2.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 66.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
      "Building wheels for collected packages: termcolor, absl-py\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=c300f81710e49c0c549716e405838e2ead90ee65b257d40717effbec8b687d4a\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=a1cfe1d0fcaa6514e0c880ea0b37730242ddf92a8523425f54d40bcba4222228\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/cc/af/1a/498a24d0730ef484019e007bb9e8cef3ac00311a672c049a3e\n",
      "Successfully built termcolor absl-py\n",
      "Installing collected packages: gast, keras-preprocessing, astunparse, google-pasta, termcolor, tensorflow-estimator, markdown, grpcio, cachetools, pyasn1-modules, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, absl-py, tensorboard, opt-einsum, tensorflow\n",
      "Successfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.0 gast-0.3.3 google-auth-1.15.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.29.0 keras-preprocessing-1.1.2 markdown-3.2.2 oauthlib-3.1.0 opt-einsum-3.2.1 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 tensorboard-2.2.1 tensorboard-plugin-wit-1.6.0.post3 tensorflow-2.2.0 tensorflow-estimator-2.2.0 termcolor-1.1.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\n",
      "\u001b[K     |████████████████████████████████| 195 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from tensorboardX) (1.14.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from tensorboardX) (1.18.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from tensorboardX) (3.11.4)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorboardX) (46.1.3.post20200330)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboardX'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b0c68bbee69e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip3 install tensorflow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip3 install tensorboardX'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboardX'"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow\n",
    "!pip3 install tensorboardX\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir=\"./data/kinetics_videos/\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"./video_download/download.py\", line 11, in <module>\r\n",
      "    import argparse\r\n",
      "ImportError: No module named argparse\r\n"
     ]
    }
   ],
   "source": [
    "!python2 ./video_download/download.py ./video_download/kinetics-400_val_8videos.csv ./data/kinetics_videos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: argparse in /home/ec2-user/anaconda3/lib/python3.7/site-packages (1.4.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"./video_download/download.py\", line 11, in <module>\r\n",
      "    import argparse\r\n",
      "ImportError: No module named argparse\r\n"
     ]
    }
   ],
   "source": [
    "!python2 ./video_download/download.py ./video_download/kinetics-400_val_8videos.csv ./data/kinetics_videos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: | \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - defaults/noarch::numpydoc==0.9.2=py_0\n",
      "  - defaults/noarch::sphinx==2.4.4=py_0\n",
      "  - defaults/noarch::s3fs==0.4.0=py_0\n",
      "  - defaults/linux-64::spyder==4.1.2=py36_0\n",
      "done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/pytorch_p36\n",
      "\n",
      "  added / updated specs:\n",
      "    - ffmpeg\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    boto3-1.13.13              |     pyh9f0ad1d_0          69 KB  conda-forge\n",
      "    botocore-1.16.13           |     pyh9f0ad1d_0         3.8 MB  conda-forge\n",
      "    docutils-0.15.2            |           py36_0         736 KB  conda-forge\n",
      "    ffmpeg-4.2                 |       h167e202_0        80.2 MB  conda-forge\n",
      "    s3transfer-0.3.3           |   py36h9f0ad1d_1          90 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        85.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  boto3              conda-forge/noarch::boto3-1.13.13-pyh9f0ad1d_0\n",
      "  botocore           conda-forge/noarch::botocore-1.16.13-pyh9f0ad1d_0\n",
      "  docutils           conda-forge/linux-64::docutils-0.15.2-py36_0\n",
      "  ffmpeg             conda-forge/linux-64::ffmpeg-4.2-h167e202_0\n",
      "  gnutls             conda-forge/linux-64::gnutls-3.6.5-hd3a4fd2_1002\n",
      "  lame               conda-forge/linux-64::lame-3.100-h14c3975_1001\n",
      "  libiconv           conda-forge/linux-64::libiconv-1.15-h516909a_1006\n",
      "  nettle             conda-forge/linux-64::nettle-3.4.1-h1bed415_1002\n",
      "  openh264           conda-forge/linux-64::openh264-1.8.0-hdbcaa40_1000\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.6-1_cp36m\n",
      "  s3transfer         conda-forge/linux-64::s3transfer-0.3.3-py36h9f0ad1d_1\n",
      "  x264               conda-forge/linux-64::x264-1!152.20180806-h14c3975_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates     pkgs/main::ca-certificates-2020.1.1-0 --> conda-forge::ca-certificates-2020.4.5.1-hecc5488_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi              pkgs/main::certifi-2020.4.5.1-py36_0 --> conda-forge::certifi-2020.4.5.1-py36h9f0ad1d_0\n",
      "  openssl              pkgs/main::openssl-1.1.1g-h7b6447c_0 --> conda-forge::openssl-1.1.1g-h516909a_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "botocore-1.16.13     | 3.8 MB    | ##################################### | 100% \n",
      "ffmpeg-4.2           | 80.2 MB   | ##################################### | 100% \n",
      "docutils-0.15.2      | 736 KB    | ##################################### | 100% \n",
      "boto3-1.13.13        | 69 KB     | ##################################### | 100% \n",
      "s3transfer-0.3.3     | 90 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge ffmpeg --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arm wrestling', 'bungee jumping', '.ipynb_checkpoints']\n",
      "ffmpeg -i \"./data/kinetics_videos/arm wrestling/video0520_sumou_2.mp4\" -vf scale=-1:256 \"./data/kinetics_videos/arm wrestling/video0520_sumou_2/image_%05d.jpg\"\n",
      "\n",
      "\n",
      "ffmpeg -i \"./data/kinetics_videos/arm wrestling/video0520_sumou_3.mp4\" -vf scale=-1:256 \"./data/kinetics_videos/arm wrestling/video0520_sumou_3/image_%05d.jpg\"\n",
      "\n",
      "\n",
      "ffmpeg -i \"./data/kinetics_videos/arm wrestling/video0520_sumou_1.mp4\" -vf scale=-1:256 \"./data/kinetics_videos/arm wrestling/video0520_sumou_1/image_%05d.jpg\"\n",
      "\n",
      "\n",
      "ffmpeg -i \"./data/kinetics_videos/bungee jumping/video0520_3.mp4\" -vf scale=-1:256 \"./data/kinetics_videos/bungee jumping/video0520_3/image_%05d.jpg\"\n",
      "\n",
      "\n",
      "ffmpeg -i \"./data/kinetics_videos/bungee jumping/video0520.mp4\" -vf scale=-1:256 \"./data/kinetics_videos/bungee jumping/video0520/image_%05d.jpg\"\n",
      "\n",
      "\n",
      "ffmpeg -i \"./data/kinetics_videos/bungee jumping/video0520_2.mp4\" -vf scale=-1:256 \"./data/kinetics_videos/bungee jumping/video0520_2/image_%05d.jpg\"\n",
      "\n",
      "\n",
      "動画ファイルを画像ファイルに変換しました。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess  # ターミナルで実行するコマンドを実行できる\n",
    "\n",
    "# 動画が保存されたフォルダ「kinetics_videos」にある、クラスの種類とパスを取得\n",
    "dir_path = './data/kinetics_videos'\n",
    "class_list = os.listdir(path=dir_path)\n",
    "print(class_list)\n",
    "\n",
    "# 各クラスの動画ファイルを画像ファイルに変換する\n",
    "for class_list_i in (class_list):  # クラスごとのループ\n",
    "\n",
    "    # クラスのフォルダへのパスを取得\n",
    "    class_path = os.path.join(dir_path, class_list_i)\n",
    "\n",
    "    # 各クラスのフォルダ内の動画ファイルをひとつずつ処理するループ\n",
    "    for file_name in os.listdir(class_path):\n",
    "\n",
    "        # ファイル名と拡張子に分割\n",
    "        name, ext = os.path.splitext(file_name)\n",
    "\n",
    "        # mp4ファイルでない、フォルダなどは処理しない\n",
    "        if ext != '.mp4':\n",
    "            continue\n",
    "\n",
    "        # 動画ファイルを画像に分割して保存するフォルダ名を取得\n",
    "        dst_directory_path = os.path.join(class_path, name)\n",
    "\n",
    "        # 上記の画像保存フォルダがなければ作成\n",
    "        if not os.path.exists(dst_directory_path):\n",
    "            os.mkdir(dst_directory_path)\n",
    "\n",
    "        # 動画ファイルへのパスを取得\n",
    "        video_file_path = os.path.join(class_path, file_name)\n",
    "\n",
    "        # ffmpegを実行させ、動画ファイルをjpgにする （高さは256ピクセルで幅はアスペクト比を変えない）\n",
    "        # kineticsの動画の場合10秒になっており、大体300ファイルになる（30 frames /sec）\n",
    "        cmd = 'ffmpeg -i \\\"{}\\\" -vf scale=-1:256 \\\"{}/image_%05d.jpg\\\"'.format(\n",
    "            video_file_path, dst_directory_path)\n",
    "        print(cmd)\n",
    "        subprocess.call(cmd, shell=True)\n",
    "        print('\\n')\n",
    "\n",
    "print(\"動画ファイルを画像ファイルに変換しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/kinetics_videos/arm wrestling/video0520_sumou_2\n",
      "./data/kinetics_videos/arm wrestling/video0520_sumou_3\n"
     ]
    }
   ],
   "source": [
    "def make_datapath_list(root_path):\n",
    "    \"\"\"\n",
    "    動画を画像データにしたフォルダへのファイルパスリストを作成する。\n",
    "    root_path : str、データフォルダへのrootパス\n",
    "    Returns：ret : video_list、動画を画像データにしたフォルダへのファイルパスリスト\n",
    "    \"\"\"\n",
    "\n",
    "    # 動画を画像データにしたフォルダへのファイルパスリスト\n",
    "    video_list = list()\n",
    "\n",
    "    # root_pathにある、クラスの種類とパスを取得\n",
    "    class_list = os.listdir(path=root_path)\n",
    "\n",
    "    # 各クラスの動画ファイルを画像化したフォルダへのパスを取得\n",
    "    for class_list_i in (class_list):  # クラスごとのループ\n",
    "\n",
    "        # クラスのフォルダへのパスを取得\n",
    "        class_path = os.path.join(root_path, class_list_i)\n",
    "\n",
    "        # 各クラスのフォルダ内の画像フォルダを取得するループ\n",
    "        for file_name in os.listdir(class_path):\n",
    "\n",
    "            # ファイル名と拡張子に分割\n",
    "            name, ext = os.path.splitext(file_name)\n",
    "\n",
    "            # フォルダでないmp4ファイルは無視\n",
    "            if ext == '.mp4':\n",
    "                continue\n",
    "\n",
    "            # 動画ファイルを画像に分割して保存したフォルダのパスを取得\n",
    "            video_img_directory_path = os.path.join(class_path, name)\n",
    "\n",
    "            # vieo_listに追加\n",
    "            video_list.append(video_img_directory_path)\n",
    "\n",
    "    return video_list\n",
    "\n",
    "\n",
    "# 動作確認\n",
    "root_path = './data/kinetics_videos/'\n",
    "video_list = make_datapath_list(root_path)\n",
    "print(video_list[0])\n",
    "print(video_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoTransform():\n",
    "    \"\"\"\n",
    "    動画を画像にした画像ファイルの前処理クラス。学習時と推論時で異なる動作をします。\n",
    "    動画を画像に分割しているため、分割された画像たちをまとめて前処理する点に注意してください。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, resize, crop_size, mean, std):\n",
    "        self.data_transform = {\n",
    "            'train': torchvision.transforms.Compose([\n",
    "                # DataAugumentation()  # 今回は省略\n",
    "                GroupResize(int(resize)),  # 画像をまとめてリサイズ　\n",
    "                GroupCenterCrop(crop_size),  # 画像をまとめてセンタークロップ\n",
    "                GroupToTensor(),  # データをPyTorchのテンソルに\n",
    "                GroupImgNormalize(mean, std),  # データを標準化\n",
    "                Stack()  # 複数画像をframes次元で結合させる\n",
    "            ]),\n",
    "            'val': torchvision.transforms.Compose([\n",
    "                GroupResize(int(resize)),  # 画像をまとめてリサイズ　\n",
    "                GroupCenterCrop(crop_size),  # 画像をまとめてセンタークロップ\n",
    "                GroupToTensor(),  # データをPyTorchのテンソルに\n",
    "                GroupImgNormalize(mean, std),  # データを標準化\n",
    "                Stack()  # 複数画像をframes次元で結合させる\n",
    "            ])\n",
    "        }\n",
    "\n",
    "    def __call__(self, img_group, phase):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        phase : 'train' or 'val'\n",
    "            前処理のモードを指定。\n",
    "        \"\"\"\n",
    "        return self.data_transform[phase](img_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理で使用するクラスたちの定義\n",
    "\n",
    "\n",
    "class GroupResize():\n",
    "    ''' 画像をまとめてリスケールするクラス。\n",
    "    画像の短い方の辺の長さがresizeに変換される。\n",
    "    アスペクト比は保たれる。\n",
    "    '''\n",
    "\n",
    "    def __init__(self, resize, interpolation=Image.BILINEAR):\n",
    "        '''リスケールする処理を用意'''\n",
    "        self.rescaler = torchvision.transforms.Resize(resize, interpolation)\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        '''リスケールをimg_group(リスト)内の各imgに実施'''\n",
    "        return [self.rescaler(img) for img in img_group]\n",
    "\n",
    "\n",
    "class GroupCenterCrop():\n",
    "    ''' 画像をまとめてセンタークロップするクラス。\n",
    "        （crop_size, crop_size）の画像を切り出す。\n",
    "    '''\n",
    "\n",
    "    def __init__(self, crop_size):\n",
    "        '''センタークロップする処理を用意'''\n",
    "        self.ccrop = torchvision.transforms.CenterCrop(crop_size)\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        '''センタークロップをimg_group(リスト)内の各imgに実施'''\n",
    "        return [self.ccrop(img) for img in img_group]\n",
    "\n",
    "\n",
    "class GroupToTensor():\n",
    "    ''' 画像をまとめてテンソル化するクラス。\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''テンソル化する処理を用意'''\n",
    "        self.to_tensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        '''テンソル化をimg_group(リスト)内の各imgに実施\n",
    "        0から1ではなく、0から255で扱うため、255をかけ算する。\n",
    "        0から255で扱うのは、学習済みデータの形式に合わせるため\n",
    "        '''\n",
    "\n",
    "        return [self.to_tensor(img)*255 for img in img_group]\n",
    "\n",
    "\n",
    "class GroupImgNormalize():\n",
    "    ''' 画像をまとめて標準化するクラス。\n",
    "    '''\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        '''標準化する処理を用意'''\n",
    "        self.normlize = torchvision.transforms.Normalize(mean, std)\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        '''標準化をimg_group(リスト)内の各imgに実施'''\n",
    "        return [self.normlize(img) for img in img_group]\n",
    "\n",
    "\n",
    "class Stack():\n",
    "    ''' 画像を一つのテンソルにまとめるクラス。\n",
    "    '''\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        '''img_groupはtorch.Size([3, 224, 224])を要素とするリスト\n",
    "        '''\n",
    "        ret = torch.cat([(x.flip(dims=[0])).unsqueeze(dim=0)\n",
    "                         for x in img_group], dim=0)  # frames次元で結合\n",
    "        # x.flip(dims=[0])は色チャネルをRGBからBGRへと順番を変えています（元の学習データがBGRであったため）\n",
    "        # unsqueeze(dim=0)はあらたにframes用の次元を作成しています\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abseiling': 0,\n",
       " 'air drumming': 1,\n",
       " 'answering questions': 2,\n",
       " 'applauding': 3,\n",
       " 'applying cream': 4,\n",
       " 'archery': 5,\n",
       " 'arm wrestling': 6,\n",
       " 'arranging flowers': 7,\n",
       " 'assembling computer': 8,\n",
       " 'auctioning': 9,\n",
       " 'baby waking up': 10,\n",
       " 'baking cookies': 11,\n",
       " 'balloon blowing': 12,\n",
       " 'bandaging': 13,\n",
       " 'barbequing': 14,\n",
       " 'bartending': 15,\n",
       " 'beatboxing': 16,\n",
       " 'bee keeping': 17,\n",
       " 'belly dancing': 18,\n",
       " 'bench pressing': 19,\n",
       " 'bending back': 20,\n",
       " 'bending metal': 21,\n",
       " 'biking through snow': 22,\n",
       " 'blasting sand': 23,\n",
       " 'blowing glass': 24,\n",
       " 'blowing leaves': 25,\n",
       " 'blowing nose': 26,\n",
       " 'blowing out candles': 27,\n",
       " 'bobsledding': 28,\n",
       " 'bookbinding': 29,\n",
       " 'bouncing on trampoline': 30,\n",
       " 'bowling': 31,\n",
       " 'braiding hair': 32,\n",
       " 'breading or breadcrumbing': 33,\n",
       " 'breakdancing': 34,\n",
       " 'brush painting': 35,\n",
       " 'brushing hair': 36,\n",
       " 'brushing teeth': 37,\n",
       " 'building cabinet': 38,\n",
       " 'building shed': 39,\n",
       " 'bungee jumping': 40,\n",
       " 'busking': 41,\n",
       " 'canoeing or kayaking': 42,\n",
       " 'capoeira': 43,\n",
       " 'carrying baby': 44,\n",
       " 'cartwheeling': 45,\n",
       " 'carving pumpkin': 46,\n",
       " 'catching fish': 47,\n",
       " 'catching or throwing baseball': 48,\n",
       " 'catching or throwing frisbee': 49,\n",
       " 'catching or throwing softball': 50,\n",
       " 'celebrating': 51,\n",
       " 'changing oil': 52,\n",
       " 'changing wheel': 53,\n",
       " 'checking tires': 54,\n",
       " 'cheerleading': 55,\n",
       " 'chopping wood': 56,\n",
       " 'clapping': 57,\n",
       " 'clay pottery making': 58,\n",
       " 'clean and jerk': 59,\n",
       " 'cleaning floor': 60,\n",
       " 'cleaning gutters': 61,\n",
       " 'cleaning pool': 62,\n",
       " 'cleaning shoes': 63,\n",
       " 'cleaning toilet': 64,\n",
       " 'cleaning windows': 65,\n",
       " 'climbing a rope': 66,\n",
       " 'climbing ladder': 67,\n",
       " 'climbing tree': 68,\n",
       " 'contact juggling': 69,\n",
       " 'cooking chicken': 70,\n",
       " 'cooking egg': 71,\n",
       " 'cooking on campfire': 72,\n",
       " 'cooking sausages': 73,\n",
       " 'counting money': 74,\n",
       " 'country line dancing': 75,\n",
       " 'cracking neck': 76,\n",
       " 'crawling baby': 77,\n",
       " 'crossing river': 78,\n",
       " 'crying': 79,\n",
       " 'curling hair': 80,\n",
       " 'cutting nails': 81,\n",
       " 'cutting pineapple': 82,\n",
       " 'cutting watermelon': 83,\n",
       " 'dancing ballet': 84,\n",
       " 'dancing charleston': 85,\n",
       " 'dancing gangnam style': 86,\n",
       " 'dancing macarena': 87,\n",
       " 'deadlifting': 88,\n",
       " 'decorating the christmas tree': 89,\n",
       " 'digging': 90,\n",
       " 'dining': 91,\n",
       " 'disc golfing': 92,\n",
       " 'diving cliff': 93,\n",
       " 'dodgeball': 94,\n",
       " 'doing aerobics': 95,\n",
       " 'doing laundry': 96,\n",
       " 'doing nails': 97,\n",
       " 'drawing': 98,\n",
       " 'dribbling basketball': 99,\n",
       " 'drinking': 100,\n",
       " 'drinking beer': 101,\n",
       " 'drinking shots': 102,\n",
       " 'driving car': 103,\n",
       " 'driving tractor': 104,\n",
       " 'drop kicking': 105,\n",
       " 'drumming fingers': 106,\n",
       " 'dunking basketball': 107,\n",
       " 'dying hair': 108,\n",
       " 'eating burger': 109,\n",
       " 'eating cake': 110,\n",
       " 'eating carrots': 111,\n",
       " 'eating chips': 112,\n",
       " 'eating doughnuts': 113,\n",
       " 'eating hotdog': 114,\n",
       " 'eating ice cream': 115,\n",
       " 'eating spaghetti': 116,\n",
       " 'eating watermelon': 117,\n",
       " 'egg hunting': 118,\n",
       " 'exercising arm': 119,\n",
       " 'exercising with an exercise ball': 120,\n",
       " 'extinguishing fire': 121,\n",
       " 'faceplanting': 122,\n",
       " 'feeding birds': 123,\n",
       " 'feeding fish': 124,\n",
       " 'feeding goats': 125,\n",
       " 'filling eyebrows': 126,\n",
       " 'finger snapping': 127,\n",
       " 'fixing hair': 128,\n",
       " 'flipping pancake': 129,\n",
       " 'flying kite': 130,\n",
       " 'folding clothes': 131,\n",
       " 'folding napkins': 132,\n",
       " 'folding paper': 133,\n",
       " 'front raises': 134,\n",
       " 'frying vegetables': 135,\n",
       " 'garbage collecting': 136,\n",
       " 'gargling': 137,\n",
       " 'getting a haircut': 138,\n",
       " 'getting a tattoo': 139,\n",
       " 'giving or receiving award': 140,\n",
       " 'golf chipping': 141,\n",
       " 'golf driving': 142,\n",
       " 'golf putting': 143,\n",
       " 'grinding meat': 144,\n",
       " 'grooming dog': 145,\n",
       " 'grooming horse': 146,\n",
       " 'gymnastics tumbling': 147,\n",
       " 'hammer throw': 148,\n",
       " 'headbanging': 149,\n",
       " 'headbutting': 150,\n",
       " 'high jump': 151,\n",
       " 'high kick': 152,\n",
       " 'hitting baseball': 153,\n",
       " 'hockey stop': 154,\n",
       " 'holding snake': 155,\n",
       " 'hopscotch': 156,\n",
       " 'hoverboarding': 157,\n",
       " 'hugging': 158,\n",
       " 'hula hooping': 159,\n",
       " 'hurdling': 160,\n",
       " 'hurling (sport)': 161,\n",
       " 'ice climbing': 162,\n",
       " 'ice fishing': 163,\n",
       " 'ice skating': 164,\n",
       " 'ironing': 165,\n",
       " 'javelin throw': 166,\n",
       " 'jetskiing': 167,\n",
       " 'jogging': 168,\n",
       " 'juggling balls': 169,\n",
       " 'juggling fire': 170,\n",
       " 'juggling soccer ball': 171,\n",
       " 'jumping into pool': 172,\n",
       " 'jumpstyle dancing': 173,\n",
       " 'kicking field goal': 174,\n",
       " 'kicking soccer ball': 175,\n",
       " 'kissing': 176,\n",
       " 'kitesurfing': 177,\n",
       " 'knitting': 178,\n",
       " 'krumping': 179,\n",
       " 'laughing': 180,\n",
       " 'laying bricks': 181,\n",
       " 'long jump': 182,\n",
       " 'lunge': 183,\n",
       " 'making a cake': 184,\n",
       " 'making a sandwich': 185,\n",
       " 'making bed': 186,\n",
       " 'making jewelry': 187,\n",
       " 'making pizza': 188,\n",
       " 'making snowman': 189,\n",
       " 'making sushi': 190,\n",
       " 'making tea': 191,\n",
       " 'marching': 192,\n",
       " 'massaging back': 193,\n",
       " 'massaging feet': 194,\n",
       " 'massaging legs': 195,\n",
       " \"massaging person's head\": 196,\n",
       " 'milking cow': 197,\n",
       " 'mopping floor': 198,\n",
       " 'motorcycling': 199,\n",
       " 'moving furniture': 200,\n",
       " 'mowing lawn': 201,\n",
       " 'news anchoring': 202,\n",
       " 'opening bottle': 203,\n",
       " 'opening present': 204,\n",
       " 'paragliding': 205,\n",
       " 'parasailing': 206,\n",
       " 'parkour': 207,\n",
       " 'passing American football (in game)': 208,\n",
       " 'passing American football (not in game)': 209,\n",
       " 'peeling apples': 210,\n",
       " 'peeling potatoes': 211,\n",
       " 'petting animal (not cat)': 212,\n",
       " 'petting cat': 213,\n",
       " 'picking fruit': 214,\n",
       " 'planting trees': 215,\n",
       " 'plastering': 216,\n",
       " 'playing accordion': 217,\n",
       " 'playing badminton': 218,\n",
       " 'playing bagpipes': 219,\n",
       " 'playing basketball': 220,\n",
       " 'playing bass guitar': 221,\n",
       " 'playing cards': 222,\n",
       " 'playing cello': 223,\n",
       " 'playing chess': 224,\n",
       " 'playing clarinet': 225,\n",
       " 'playing controller': 226,\n",
       " 'playing cricket': 227,\n",
       " 'playing cymbals': 228,\n",
       " 'playing didgeridoo': 229,\n",
       " 'playing drums': 230,\n",
       " 'playing flute': 231,\n",
       " 'playing guitar': 232,\n",
       " 'playing harmonica': 233,\n",
       " 'playing harp': 234,\n",
       " 'playing ice hockey': 235,\n",
       " 'playing keyboard': 236,\n",
       " 'playing kickball': 237,\n",
       " 'playing monopoly': 238,\n",
       " 'playing organ': 239,\n",
       " 'playing paintball': 240,\n",
       " 'playing piano': 241,\n",
       " 'playing poker': 242,\n",
       " 'playing recorder': 243,\n",
       " 'playing saxophone': 244,\n",
       " 'playing squash or racquetball': 245,\n",
       " 'playing tennis': 246,\n",
       " 'playing trombone': 247,\n",
       " 'playing trumpet': 248,\n",
       " 'playing ukulele': 249,\n",
       " 'playing violin': 250,\n",
       " 'playing volleyball': 251,\n",
       " 'playing xylophone': 252,\n",
       " 'pole vault': 253,\n",
       " 'presenting weather forecast': 254,\n",
       " 'pull ups': 255,\n",
       " 'pumping fist': 256,\n",
       " 'pumping gas': 257,\n",
       " 'punching bag': 258,\n",
       " 'punching person (boxing)': 259,\n",
       " 'push up': 260,\n",
       " 'pushing car': 261,\n",
       " 'pushing cart': 262,\n",
       " 'pushing wheelchair': 263,\n",
       " 'reading book': 264,\n",
       " 'reading newspaper': 265,\n",
       " 'recording music': 266,\n",
       " 'riding a bike': 267,\n",
       " 'riding camel': 268,\n",
       " 'riding elephant': 269,\n",
       " 'riding mechanical bull': 270,\n",
       " 'riding mountain bike': 271,\n",
       " 'riding mule': 272,\n",
       " 'riding or walking with horse': 273,\n",
       " 'riding scooter': 274,\n",
       " 'riding unicycle': 275,\n",
       " 'ripping paper': 276,\n",
       " 'robot dancing': 277,\n",
       " 'rock climbing': 278,\n",
       " 'rock scissors paper': 279,\n",
       " 'roller skating': 280,\n",
       " 'running on treadmill': 281,\n",
       " 'sailing': 282,\n",
       " 'salsa dancing': 283,\n",
       " 'sanding floor': 284,\n",
       " 'scrambling eggs': 285,\n",
       " 'scuba diving': 286,\n",
       " 'setting table': 287,\n",
       " 'shaking hands': 288,\n",
       " 'shaking head': 289,\n",
       " 'sharpening knives': 290,\n",
       " 'sharpening pencil': 291,\n",
       " 'shaving head': 292,\n",
       " 'shaving legs': 293,\n",
       " 'shearing sheep': 294,\n",
       " 'shining shoes': 295,\n",
       " 'shooting basketball': 296,\n",
       " 'shooting goal (soccer)': 297,\n",
       " 'shot put': 298,\n",
       " 'shoveling snow': 299,\n",
       " 'shredding paper': 300,\n",
       " 'shuffling cards': 301,\n",
       " 'side kick': 302,\n",
       " 'sign language interpreting': 303,\n",
       " 'singing': 304,\n",
       " 'situp': 305,\n",
       " 'skateboarding': 306,\n",
       " 'ski jumping': 307,\n",
       " 'skiing (not slalom or crosscountry)': 308,\n",
       " 'skiing crosscountry': 309,\n",
       " 'skiing slalom': 310,\n",
       " 'skipping rope': 311,\n",
       " 'skydiving': 312,\n",
       " 'slacklining': 313,\n",
       " 'slapping': 314,\n",
       " 'sled dog racing': 315,\n",
       " 'smoking': 316,\n",
       " 'smoking hookah': 317,\n",
       " 'snatch weight lifting': 318,\n",
       " 'sneezing': 319,\n",
       " 'sniffing': 320,\n",
       " 'snorkeling': 321,\n",
       " 'snowboarding': 322,\n",
       " 'snowkiting': 323,\n",
       " 'snowmobiling': 324,\n",
       " 'somersaulting': 325,\n",
       " 'spinning poi': 326,\n",
       " 'spray painting': 327,\n",
       " 'spraying': 328,\n",
       " 'springboard diving': 329,\n",
       " 'squat': 330,\n",
       " 'sticking tongue out': 331,\n",
       " 'stomping grapes': 332,\n",
       " 'stretching arm': 333,\n",
       " 'stretching leg': 334,\n",
       " 'strumming guitar': 335,\n",
       " 'surfing crowd': 336,\n",
       " 'surfing water': 337,\n",
       " 'sweeping floor': 338,\n",
       " 'swimming backstroke': 339,\n",
       " 'swimming breast stroke': 340,\n",
       " 'swimming butterfly stroke': 341,\n",
       " 'swing dancing': 342,\n",
       " 'swinging legs': 343,\n",
       " 'swinging on something': 344,\n",
       " 'sword fighting': 345,\n",
       " 'tai chi': 346,\n",
       " 'taking a shower': 347,\n",
       " 'tango dancing': 348,\n",
       " 'tap dancing': 349,\n",
       " 'tapping guitar': 350,\n",
       " 'tapping pen': 351,\n",
       " 'tasting beer': 352,\n",
       " 'tasting food': 353,\n",
       " 'testifying': 354,\n",
       " 'texting': 355,\n",
       " 'throwing axe': 356,\n",
       " 'throwing ball': 357,\n",
       " 'throwing discus': 358,\n",
       " 'tickling': 359,\n",
       " 'tobogganing': 360,\n",
       " 'tossing coin': 361,\n",
       " 'tossing salad': 362,\n",
       " 'training dog': 363,\n",
       " 'trapezing': 364,\n",
       " 'trimming or shaving beard': 365,\n",
       " 'trimming trees': 366,\n",
       " 'triple jump': 367,\n",
       " 'tying bow tie': 368,\n",
       " 'tying knot (not on a tie)': 369,\n",
       " 'tying tie': 370,\n",
       " 'unboxing': 371,\n",
       " 'unloading truck': 372,\n",
       " 'using computer': 373,\n",
       " 'using remote controller (not gaming)': 374,\n",
       " 'using segway': 375,\n",
       " 'vault': 376,\n",
       " 'waiting in line': 377,\n",
       " 'walking the dog': 378,\n",
       " 'washing dishes': 379,\n",
       " 'washing feet': 380,\n",
       " 'washing hair': 381,\n",
       " 'washing hands': 382,\n",
       " 'water skiing': 383,\n",
       " 'water sliding': 384,\n",
       " 'watering plants': 385,\n",
       " 'waxing back': 386,\n",
       " 'waxing chest': 387,\n",
       " 'waxing eyebrows': 388,\n",
       " 'waxing legs': 389,\n",
       " 'weaving basket': 390,\n",
       " 'welding': 391,\n",
       " 'whistling': 392,\n",
       " 'windsurfing': 393,\n",
       " 'wrapping present': 394,\n",
       " 'wrestling': 395,\n",
       " 'writing': 396,\n",
       " 'yawning': 397,\n",
       " 'yoga': 398,\n",
       " 'zumba': 399}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kinetics-400のラベル名をIDに変換する辞書と、逆にIDをラベル名に変換する辞書を用意\n",
    "\n",
    "\n",
    "def get_label_id_dictionary(label_dicitionary_path='./video_download/kinetics_400_label_dicitionary.csv'):\n",
    "    label_id_dict = {}\n",
    "    id_label_dict = {}\n",
    "\n",
    "    with open(label_dicitionary_path, encoding=\"utf-8_sig\") as f:\n",
    "\n",
    "        # 読み込む\n",
    "        reader = csv.DictReader(f, delimiter=\",\", quotechar='\"')\n",
    "\n",
    "        # 1行ずつ読み込み、辞書型変数に追加します\n",
    "        for row in reader:\n",
    "            label_id_dict.setdefault(\n",
    "                row[\"class_label\"], int(row[\"label_id\"])-1)\n",
    "            id_label_dict.setdefault(\n",
    "                int(row[\"label_id\"])-1, row[\"class_label\"])\n",
    "\n",
    "    return label_id_dict,  id_label_dict\n",
    "\n",
    "\n",
    "# 確認\n",
    "label_dicitionary_path = './video_download/kinetics_400_label_dicitionary.csv'\n",
    "label_id_dict, id_label_dict = get_label_id_dictionary(label_dicitionary_path)\n",
    "label_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    動画のDataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, video_list, label_id_dict, num_segments, phase, transform, img_tmpl='image_{:05d}.jpg'):\n",
    "        self.video_list = video_list  # 動画画像のフォルダへのパスリスト\n",
    "        self.label_id_dict = label_id_dict  # ラベル名をidに変換する辞書型変数\n",
    "        self.num_segments = num_segments  # 動画を何分割して使用するのかを決める\n",
    "        self.phase = phase  # train or val\n",
    "        self.transform = transform  # 前処理\n",
    "        self.img_tmpl = img_tmpl  # 読み込みたい画像のファイル名のテンプレート\n",
    "\n",
    "    def __len__(self):\n",
    "        '''動画の数を返す'''\n",
    "        return len(self.video_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        前処理をした画像たちのデータとラベル、ラベルIDを取得\n",
    "        '''\n",
    "        imgs_transformed, label, label_id, dir_path = self.pull_item(index)\n",
    "        return imgs_transformed, label, label_id, dir_path\n",
    "\n",
    "    def pull_item(self, index):\n",
    "        '''前処理をした画像たちのデータとラベル、ラベルIDを取得'''\n",
    "\n",
    "        # 1. 画像たちをリストに読み込む\n",
    "        dir_path = self.video_list[index]  # 画像が格納されたフォルダ\n",
    "        indices = self._get_indices(dir_path)  # 読み込む画像idxを求める\n",
    "        img_group = self._load_imgs(\n",
    "            dir_path, self.img_tmpl, indices)  # リストに読み込む\n",
    "\n",
    "        # 2. ラベルの取得し、idに変換する\n",
    "        label = (dir_path.split('/')[3].split('/')[0])\n",
    "        label_id = self.label_id_dict[label] # idを取得\n",
    "\n",
    "        # 3. 前処理を実施\n",
    "        imgs_transformed = self.transform(img_group, phase=self.phase)\n",
    "\n",
    "        return imgs_transformed, label, label_id, dir_path\n",
    "\n",
    "    def _load_imgs(self, dir_path, img_tmpl, indices):\n",
    "        '''画像をまとめて読み込み、リスト化する関数'''\n",
    "        img_group = []  # 画像を格納するリスト\n",
    "\n",
    "        for idx in indices:\n",
    "            # 画像のパスを取得\n",
    "            file_path = os.path.join(dir_path, img_tmpl.format(idx))\n",
    "\n",
    "            # 画像を読み込む\n",
    "            img = Image.open(file_path).convert('RGB')\n",
    "\n",
    "            # リストに追加\n",
    "            img_group.append(img)\n",
    "        return img_group\n",
    "\n",
    "    def _get_indices(self, dir_path):\n",
    "        \"\"\"\n",
    "        動画全体をself.num_segmentに分割した際に取得する動画のidxのリストを取得する\n",
    "        \"\"\"\n",
    "        # 動画のフレーム数を求める\n",
    "        file_list = os.listdir(path=dir_path)\n",
    "        num_frames = len(file_list)\n",
    "\n",
    "        # 動画の取得間隔幅を求める\n",
    "        tick = (num_frames) / float(self.num_segments)\n",
    "        # 250 / 16 = 15.625\n",
    "        # 動画の取得間隔幅で取り出す際のidxをリストで求める\n",
    "        indices = np.array([int(tick / 2.0 + tick * x)\n",
    "                            for x in range(self.num_segments)])+1\n",
    "        # 250frameで16frame抽出の場合\n",
    "        # indices = [  8  24  40  55  71  86 102 118 133 149 165 180 196 211 227 243]\n",
    "\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224])\n",
      "arm wrestling\n",
      "6\n",
      "./data/kinetics_videos/arm wrestling/video0520_sumou_2\n"
     ]
    }
   ],
   "source": [
    "# 動作確認\n",
    "\n",
    "# vieo_listの作成\n",
    "root_path = './data/kinetics_videos/'\n",
    "video_list = make_datapath_list(root_path)\n",
    "\n",
    "# 前処理の設定\n",
    "resize, crop_size = 224, 224\n",
    "mean, std = [104, 117, 123], [1, 1, 1]\n",
    "video_transform = VideoTransform(resize, crop_size, mean, std)\n",
    "\n",
    "# Datasetの作成\n",
    "# num_segments は 動画を何分割して使用するのかを決める\n",
    "val_dataset = VideoDataset(video_list, label_id_dict, num_segments=16,\n",
    "                           phase=\"val\", transform=video_transform, img_tmpl='image_{:05d}.jpg')\n",
    "\n",
    "# データの取り出し例\n",
    "# 出力は、imgs_transformed, label, label_id, dir_path\n",
    "index = 0\n",
    "print(val_dataset.__getitem__(index)[0].shape)  # 画像たちのテンソル\n",
    "print(val_dataset.__getitem__(index)[1])  # ラベル名\n",
    "print(val_dataset.__getitem__(index)[2])  # ラベルID\n",
    "print(val_dataset.__getitem__(index)[3])  # 動画へのパス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 16, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# DataLoaderにします\n",
    "batch_size = 8\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 動作確認\n",
    "batch_iterator = iter(val_dataloader)  # イテレータに変換\n",
    "imgs_transformeds, labels, label_ids, dir_path = next(\n",
    "    batch_iterator)  # 1番目の要素を取り出す\n",
    "print(imgs_transformeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eco import ECO_2D, ECO_3D\n",
    "\n",
    "class ECO_Lite(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECO_Lite, self).__init__()\n",
    "\n",
    "        # 2D Netモジュール\n",
    "        self.eco_2d = ECO_2D()\n",
    "\n",
    "        # 3D Netモジュール\n",
    "        self.eco_3d = ECO_3D()\n",
    "\n",
    "        # クラス分類の全結合層\n",
    "        self.fc_final = nn.Linear(in_features=512, out_features=400, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        入力xはtorch.Size([batch_num, num_segments=16, 3, 224, 224]))\n",
    "        '''\n",
    "\n",
    "        # 入力xの各次元のサイズを取得する\n",
    "        bs, ns, c, h, w = x.shape\n",
    "\n",
    "        # xを(bs*ns, c, h, w)にサイズ変換する\n",
    "        out = x.view(-1, c, h, w)\n",
    "        # （注釈）\n",
    "        # PyTorchのConv2Dは入力のサイズが(batch_num, c, h, w)しか受け付けないため\n",
    "        # (batch_num, num_segments, c, h, w)は処理できない\n",
    "        # 今は2次元画像を独立に処理するので、num_segmentsはbatch_numの次元に押し込んでも良いため\n",
    "        # (batch_num×num_segments, c, h, w)にサイズを変換する\n",
    "\n",
    "        # 2D Netモジュール 出力torch.Size([batch_num×16, 96, 28, 28])\n",
    "        out = self.eco_2d(out)\n",
    "\n",
    "        # 2次元画像をテンソルを3次元用に変換する\n",
    "        # num_segmentsをbatch_numの次元に押し込んだものを元に戻す\n",
    "        out = out.view(-1, ns, 96, 28, 28)\n",
    "\n",
    "        # 3D Netモジュール 出力torch.Size([batch_num, 512])\n",
    "        out = self.eco_3d(out)\n",
    "\n",
    "        # クラス分類の全結合層　出力torch.Size([batch_num, class_num=400])\n",
    "        out = self.fc_final(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ECO_Lite(\n",
       "  (eco_2d): ECO_2D(\n",
       "    (basic_conv): BasicConv(\n",
       "      (conv1_7x7_s2): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "      (conv1_7x7_s2_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1_relu_7x7): ReLU(inplace=True)\n",
       "      (pool1_3x3_s2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "      (conv2_3x3_reduce): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv2_3x3_reduce_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2_relu_3x3_reduce): ReLU(inplace=True)\n",
       "      (conv2_3x3): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2_3x3_bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2_relu_3x3): ReLU(inplace=True)\n",
       "      (pool2_3x3_s2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    )\n",
       "    (inception_a): InceptionA(\n",
       "      (inception_3a_1x1): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (inception_3a_1x1_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (inception_3a_relu_1x1): ReLU(inplace=True)\n",
       "      (inception_3a_3x3_reduce): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (inception_3a_3x3_reduce_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (inception_3a_relu_3x3_reduce): ReLU(inplace=True)\n",
       "      (inception_3a_3x3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (inception_3a_3x3_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (inception_3a_relu_3x3): ReLU(inplace=True)\n",
       "      (inception_3a_double_3x3_reduce): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (inception_3a_double_3x3_reduce_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (inception_3a_relu_double_3x3_reduce): ReLU(inplace=True)\n",
       "      (inception_3a_double_3x3_1): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (inception_3a_double_3x3_1_bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (inception_3a_relu_double_3x3_1): ReLU(inplace=True)\n",
       "      (inception_3a_double_3x3_2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (inception_3a_double_3x3_2_bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (inception_3a_relu_double_3x3_2): ReLU(inplace=True)\n",
       "      (inception_3a_pool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "      (inception_3a_pool_proj): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (inception_3a_pool_proj_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (inception_3a_relu_pool_proj): ReLU(inplace=True)\n",
       "    )\n",
       "    (inception_b): InceptionB(\n",
       "      (inception_3b_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (inception_3b_1x1_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (inception_3b_relu_1x1): ReLU(inplace=True)\n",
       "      (inception_3b_3x3_reduce): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (inception_3b_3x3_reduce_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (inception_3b_relu_3x3_reduce): ReLU(inplace=True)\n",
       "      (inception_3b_3x3): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (inception_3b_3x3_bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (inception_3b_relu_3x3): ReLU(inplace=True)\n",
       "      (inception_3b_double_3x3_reduce): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (inception_3b_double_3x3_reduce_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (inception_3b_relu_double_3x3_reduce): ReLU(inplace=True)\n",
       "      (inception_3b_double_3x3_1): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (inception_3b_double_3x3_1_bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (inception_3b_relu_double_3x3_1): ReLU(inplace=True)\n",
       "      (inception_3b_double_3x3_2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (inception_3b_double_3x3_2_bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (inception_3b_relu_double_3x3_2): ReLU(inplace=True)\n",
       "      (inception_3b_pool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "      (inception_3b_pool_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (inception_3b_pool_proj_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (inception_3b_relu_pool_proj): ReLU(inplace=True)\n",
       "    )\n",
       "    (inception_c): InceptionC(\n",
       "      (inception_3c_double_3x3_reduce): Conv2d(320, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (inception_3c_double_3x3_reduce_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (inception_3c_relu_double_3x3_reduce): ReLU(inplace=True)\n",
       "      (inception_3c_double_3x3_1): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (inception_3c_double_3x3_1_bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (inception_3c_relu_double_3x3_1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (eco_3d): ECO_3D(\n",
       "    (res_3d_3): Resnet_3D_3(\n",
       "      (res3a_2): Conv3d(96, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (res3a_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (res3a_relu): ReLU(inplace=True)\n",
       "      (res3b_1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (res3b_1_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (res3b_1_relu): ReLU(inplace=True)\n",
       "      (res3b_2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (res3b_bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (res3b_relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (res_3d_4): Resnet_3D_4(\n",
       "      (res4a_1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (res4a_1_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (res4a_1_relu): ReLU(inplace=True)\n",
       "      (res4a_2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (res4a_down): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (res4a_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (res4a_relu): ReLU(inplace=True)\n",
       "      (res4b_1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (res4b_1_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (res4b_1_relu): ReLU(inplace=True)\n",
       "      (res4b_2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (res4b_bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (res4b_relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (res_3d_5): Resnet_3D_5(\n",
       "      (res5a_1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (res5a_1_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (res5a_1_relu): ReLU(inplace=True)\n",
       "      (res5a_2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (res5a_down): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (res5a_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (res5a_relu): ReLU(inplace=True)\n",
       "      (res5b_1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (res5b_1_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (res5b_1_relu): ReLU(inplace=True)\n",
       "      (res5b_2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (res5b_bn): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (res5b_relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (global_pool): AvgPool3d(kernel_size=(4, 7, 7), stride=1, padding=0)\n",
       "  )\n",
       "  (fc_final): Linear(in_features=512, out_features=400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ECO_Lite()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"./weights/\"\n",
    "if not os.path.exists(weights_dir):\n",
    "    os.mkdir(weights_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習済みのパラメータをロードします\n",
      "module.base_model.conv1_7x7_s2.weight→eco_2d.basic_conv.conv1_7x7_s2.weight\n",
      "module.base_model.conv1_7x7_s2.bias→eco_2d.basic_conv.conv1_7x7_s2.bias\n",
      "module.base_model.conv1_7x7_s2_bn.weight→eco_2d.basic_conv.conv1_7x7_s2_bn.weight\n",
      "module.base_model.conv1_7x7_s2_bn.bias→eco_2d.basic_conv.conv1_7x7_s2_bn.bias\n",
      "module.base_model.conv1_7x7_s2_bn.running_mean→eco_2d.basic_conv.conv1_7x7_s2_bn.running_mean\n",
      "module.base_model.conv1_7x7_s2_bn.running_var→eco_2d.basic_conv.conv1_7x7_s2_bn.running_var\n",
      "module.base_model.conv1_7x7_s2_bn.num_batches_tracked→eco_2d.basic_conv.conv1_7x7_s2_bn.num_batches_tracked\n",
      "module.base_model.conv2_3x3_reduce.weight→eco_2d.basic_conv.conv2_3x3_reduce.weight\n",
      "module.base_model.conv2_3x3_reduce.bias→eco_2d.basic_conv.conv2_3x3_reduce.bias\n",
      "module.base_model.conv2_3x3_reduce_bn.weight→eco_2d.basic_conv.conv2_3x3_reduce_bn.weight\n",
      "module.base_model.conv2_3x3_reduce_bn.bias→eco_2d.basic_conv.conv2_3x3_reduce_bn.bias\n",
      "module.base_model.conv2_3x3_reduce_bn.running_mean→eco_2d.basic_conv.conv2_3x3_reduce_bn.running_mean\n",
      "module.base_model.conv2_3x3_reduce_bn.running_var→eco_2d.basic_conv.conv2_3x3_reduce_bn.running_var\n",
      "module.base_model.conv2_3x3_reduce_bn.num_batches_tracked→eco_2d.basic_conv.conv2_3x3_reduce_bn.num_batches_tracked\n",
      "module.base_model.conv2_3x3.weight→eco_2d.basic_conv.conv2_3x3.weight\n",
      "module.base_model.conv2_3x3.bias→eco_2d.basic_conv.conv2_3x3.bias\n",
      "module.base_model.conv2_3x3_bn.weight→eco_2d.basic_conv.conv2_3x3_bn.weight\n",
      "module.base_model.conv2_3x3_bn.bias→eco_2d.basic_conv.conv2_3x3_bn.bias\n",
      "module.base_model.conv2_3x3_bn.running_mean→eco_2d.basic_conv.conv2_3x3_bn.running_mean\n",
      "module.base_model.conv2_3x3_bn.running_var→eco_2d.basic_conv.conv2_3x3_bn.running_var\n",
      "module.base_model.conv2_3x3_bn.num_batches_tracked→eco_2d.basic_conv.conv2_3x3_bn.num_batches_tracked\n",
      "module.base_model.inception_3a_1x1.weight→eco_2d.inception_a.inception_3a_1x1.weight\n",
      "module.base_model.inception_3a_1x1.bias→eco_2d.inception_a.inception_3a_1x1.bias\n",
      "module.base_model.inception_3a_1x1_bn.weight→eco_2d.inception_a.inception_3a_1x1_bn.weight\n",
      "module.base_model.inception_3a_1x1_bn.bias→eco_2d.inception_a.inception_3a_1x1_bn.bias\n",
      "module.base_model.inception_3a_1x1_bn.running_mean→eco_2d.inception_a.inception_3a_1x1_bn.running_mean\n",
      "module.base_model.inception_3a_1x1_bn.running_var→eco_2d.inception_a.inception_3a_1x1_bn.running_var\n",
      "module.base_model.inception_3a_1x1_bn.num_batches_tracked→eco_2d.inception_a.inception_3a_1x1_bn.num_batches_tracked\n",
      "module.base_model.inception_3a_3x3_reduce.weight→eco_2d.inception_a.inception_3a_3x3_reduce.weight\n",
      "module.base_model.inception_3a_3x3_reduce.bias→eco_2d.inception_a.inception_3a_3x3_reduce.bias\n",
      "module.base_model.inception_3a_3x3_reduce_bn.weight→eco_2d.inception_a.inception_3a_3x3_reduce_bn.weight\n",
      "module.base_model.inception_3a_3x3_reduce_bn.bias→eco_2d.inception_a.inception_3a_3x3_reduce_bn.bias\n",
      "module.base_model.inception_3a_3x3_reduce_bn.running_mean→eco_2d.inception_a.inception_3a_3x3_reduce_bn.running_mean\n",
      "module.base_model.inception_3a_3x3_reduce_bn.running_var→eco_2d.inception_a.inception_3a_3x3_reduce_bn.running_var\n",
      "module.base_model.inception_3a_3x3_reduce_bn.num_batches_tracked→eco_2d.inception_a.inception_3a_3x3_reduce_bn.num_batches_tracked\n",
      "module.base_model.inception_3a_3x3.weight→eco_2d.inception_a.inception_3a_3x3.weight\n",
      "module.base_model.inception_3a_3x3.bias→eco_2d.inception_a.inception_3a_3x3.bias\n",
      "module.base_model.inception_3a_3x3_bn.weight→eco_2d.inception_a.inception_3a_3x3_bn.weight\n",
      "module.base_model.inception_3a_3x3_bn.bias→eco_2d.inception_a.inception_3a_3x3_bn.bias\n",
      "module.base_model.inception_3a_3x3_bn.running_mean→eco_2d.inception_a.inception_3a_3x3_bn.running_mean\n",
      "module.base_model.inception_3a_3x3_bn.running_var→eco_2d.inception_a.inception_3a_3x3_bn.running_var\n",
      "module.base_model.inception_3a_3x3_bn.num_batches_tracked→eco_2d.inception_a.inception_3a_3x3_bn.num_batches_tracked\n",
      "module.base_model.inception_3a_double_3x3_reduce.weight→eco_2d.inception_a.inception_3a_double_3x3_reduce.weight\n",
      "module.base_model.inception_3a_double_3x3_reduce.bias→eco_2d.inception_a.inception_3a_double_3x3_reduce.bias\n",
      "module.base_model.inception_3a_double_3x3_reduce_bn.weight→eco_2d.inception_a.inception_3a_double_3x3_reduce_bn.weight\n",
      "module.base_model.inception_3a_double_3x3_reduce_bn.bias→eco_2d.inception_a.inception_3a_double_3x3_reduce_bn.bias\n",
      "module.base_model.inception_3a_double_3x3_reduce_bn.running_mean→eco_2d.inception_a.inception_3a_double_3x3_reduce_bn.running_mean\n",
      "module.base_model.inception_3a_double_3x3_reduce_bn.running_var→eco_2d.inception_a.inception_3a_double_3x3_reduce_bn.running_var\n",
      "module.base_model.inception_3a_double_3x3_reduce_bn.num_batches_tracked→eco_2d.inception_a.inception_3a_double_3x3_reduce_bn.num_batches_tracked\n",
      "module.base_model.inception_3a_double_3x3_1.weight→eco_2d.inception_a.inception_3a_double_3x3_1.weight\n",
      "module.base_model.inception_3a_double_3x3_1.bias→eco_2d.inception_a.inception_3a_double_3x3_1.bias\n",
      "module.base_model.inception_3a_double_3x3_1_bn.weight→eco_2d.inception_a.inception_3a_double_3x3_1_bn.weight\n",
      "module.base_model.inception_3a_double_3x3_1_bn.bias→eco_2d.inception_a.inception_3a_double_3x3_1_bn.bias\n",
      "module.base_model.inception_3a_double_3x3_1_bn.running_mean→eco_2d.inception_a.inception_3a_double_3x3_1_bn.running_mean\n",
      "module.base_model.inception_3a_double_3x3_1_bn.running_var→eco_2d.inception_a.inception_3a_double_3x3_1_bn.running_var\n",
      "module.base_model.inception_3a_double_3x3_1_bn.num_batches_tracked→eco_2d.inception_a.inception_3a_double_3x3_1_bn.num_batches_tracked\n",
      "module.base_model.inception_3a_double_3x3_2.weight→eco_2d.inception_a.inception_3a_double_3x3_2.weight\n",
      "module.base_model.inception_3a_double_3x3_2.bias→eco_2d.inception_a.inception_3a_double_3x3_2.bias\n",
      "module.base_model.inception_3a_double_3x3_2_bn.weight→eco_2d.inception_a.inception_3a_double_3x3_2_bn.weight\n",
      "module.base_model.inception_3a_double_3x3_2_bn.bias→eco_2d.inception_a.inception_3a_double_3x3_2_bn.bias\n",
      "module.base_model.inception_3a_double_3x3_2_bn.running_mean→eco_2d.inception_a.inception_3a_double_3x3_2_bn.running_mean\n",
      "module.base_model.inception_3a_double_3x3_2_bn.running_var→eco_2d.inception_a.inception_3a_double_3x3_2_bn.running_var\n",
      "module.base_model.inception_3a_double_3x3_2_bn.num_batches_tracked→eco_2d.inception_a.inception_3a_double_3x3_2_bn.num_batches_tracked\n",
      "module.base_model.inception_3a_pool_proj.weight→eco_2d.inception_a.inception_3a_pool_proj.weight\n",
      "module.base_model.inception_3a_pool_proj.bias→eco_2d.inception_a.inception_3a_pool_proj.bias\n",
      "module.base_model.inception_3a_pool_proj_bn.weight→eco_2d.inception_a.inception_3a_pool_proj_bn.weight\n",
      "module.base_model.inception_3a_pool_proj_bn.bias→eco_2d.inception_a.inception_3a_pool_proj_bn.bias\n",
      "module.base_model.inception_3a_pool_proj_bn.running_mean→eco_2d.inception_a.inception_3a_pool_proj_bn.running_mean\n",
      "module.base_model.inception_3a_pool_proj_bn.running_var→eco_2d.inception_a.inception_3a_pool_proj_bn.running_var\n",
      "module.base_model.inception_3a_pool_proj_bn.num_batches_tracked→eco_2d.inception_a.inception_3a_pool_proj_bn.num_batches_tracked\n",
      "module.base_model.inception_3b_1x1.weight→eco_2d.inception_b.inception_3b_1x1.weight\n",
      "module.base_model.inception_3b_1x1.bias→eco_2d.inception_b.inception_3b_1x1.bias\n",
      "module.base_model.inception_3b_1x1_bn.weight→eco_2d.inception_b.inception_3b_1x1_bn.weight\n",
      "module.base_model.inception_3b_1x1_bn.bias→eco_2d.inception_b.inception_3b_1x1_bn.bias\n",
      "module.base_model.inception_3b_1x1_bn.running_mean→eco_2d.inception_b.inception_3b_1x1_bn.running_mean\n",
      "module.base_model.inception_3b_1x1_bn.running_var→eco_2d.inception_b.inception_3b_1x1_bn.running_var\n",
      "module.base_model.inception_3b_1x1_bn.num_batches_tracked→eco_2d.inception_b.inception_3b_1x1_bn.num_batches_tracked\n",
      "module.base_model.inception_3b_3x3_reduce.weight→eco_2d.inception_b.inception_3b_3x3_reduce.weight\n",
      "module.base_model.inception_3b_3x3_reduce.bias→eco_2d.inception_b.inception_3b_3x3_reduce.bias\n",
      "module.base_model.inception_3b_3x3_reduce_bn.weight→eco_2d.inception_b.inception_3b_3x3_reduce_bn.weight\n",
      "module.base_model.inception_3b_3x3_reduce_bn.bias→eco_2d.inception_b.inception_3b_3x3_reduce_bn.bias\n",
      "module.base_model.inception_3b_3x3_reduce_bn.running_mean→eco_2d.inception_b.inception_3b_3x3_reduce_bn.running_mean\n",
      "module.base_model.inception_3b_3x3_reduce_bn.running_var→eco_2d.inception_b.inception_3b_3x3_reduce_bn.running_var\n",
      "module.base_model.inception_3b_3x3_reduce_bn.num_batches_tracked→eco_2d.inception_b.inception_3b_3x3_reduce_bn.num_batches_tracked\n",
      "module.base_model.inception_3b_3x3.weight→eco_2d.inception_b.inception_3b_3x3.weight\n",
      "module.base_model.inception_3b_3x3.bias→eco_2d.inception_b.inception_3b_3x3.bias\n",
      "module.base_model.inception_3b_3x3_bn.weight→eco_2d.inception_b.inception_3b_3x3_bn.weight\n",
      "module.base_model.inception_3b_3x3_bn.bias→eco_2d.inception_b.inception_3b_3x3_bn.bias\n",
      "module.base_model.inception_3b_3x3_bn.running_mean→eco_2d.inception_b.inception_3b_3x3_bn.running_mean\n",
      "module.base_model.inception_3b_3x3_bn.running_var→eco_2d.inception_b.inception_3b_3x3_bn.running_var\n",
      "module.base_model.inception_3b_3x3_bn.num_batches_tracked→eco_2d.inception_b.inception_3b_3x3_bn.num_batches_tracked\n",
      "module.base_model.inception_3b_double_3x3_reduce.weight→eco_2d.inception_b.inception_3b_double_3x3_reduce.weight\n",
      "module.base_model.inception_3b_double_3x3_reduce.bias→eco_2d.inception_b.inception_3b_double_3x3_reduce.bias\n",
      "module.base_model.inception_3b_double_3x3_reduce_bn.weight→eco_2d.inception_b.inception_3b_double_3x3_reduce_bn.weight\n",
      "module.base_model.inception_3b_double_3x3_reduce_bn.bias→eco_2d.inception_b.inception_3b_double_3x3_reduce_bn.bias\n",
      "module.base_model.inception_3b_double_3x3_reduce_bn.running_mean→eco_2d.inception_b.inception_3b_double_3x3_reduce_bn.running_mean\n",
      "module.base_model.inception_3b_double_3x3_reduce_bn.running_var→eco_2d.inception_b.inception_3b_double_3x3_reduce_bn.running_var\n",
      "module.base_model.inception_3b_double_3x3_reduce_bn.num_batches_tracked→eco_2d.inception_b.inception_3b_double_3x3_reduce_bn.num_batches_tracked\n",
      "module.base_model.inception_3b_double_3x3_1.weight→eco_2d.inception_b.inception_3b_double_3x3_1.weight\n",
      "module.base_model.inception_3b_double_3x3_1.bias→eco_2d.inception_b.inception_3b_double_3x3_1.bias\n",
      "module.base_model.inception_3b_double_3x3_1_bn.weight→eco_2d.inception_b.inception_3b_double_3x3_1_bn.weight\n",
      "module.base_model.inception_3b_double_3x3_1_bn.bias→eco_2d.inception_b.inception_3b_double_3x3_1_bn.bias\n",
      "module.base_model.inception_3b_double_3x3_1_bn.running_mean→eco_2d.inception_b.inception_3b_double_3x3_1_bn.running_mean\n",
      "module.base_model.inception_3b_double_3x3_1_bn.running_var→eco_2d.inception_b.inception_3b_double_3x3_1_bn.running_var\n",
      "module.base_model.inception_3b_double_3x3_1_bn.num_batches_tracked→eco_2d.inception_b.inception_3b_double_3x3_1_bn.num_batches_tracked\n",
      "module.base_model.inception_3b_double_3x3_2.weight→eco_2d.inception_b.inception_3b_double_3x3_2.weight\n",
      "module.base_model.inception_3b_double_3x3_2.bias→eco_2d.inception_b.inception_3b_double_3x3_2.bias\n",
      "module.base_model.inception_3b_double_3x3_2_bn.weight→eco_2d.inception_b.inception_3b_double_3x3_2_bn.weight\n",
      "module.base_model.inception_3b_double_3x3_2_bn.bias→eco_2d.inception_b.inception_3b_double_3x3_2_bn.bias\n",
      "module.base_model.inception_3b_double_3x3_2_bn.running_mean→eco_2d.inception_b.inception_3b_double_3x3_2_bn.running_mean\n",
      "module.base_model.inception_3b_double_3x3_2_bn.running_var→eco_2d.inception_b.inception_3b_double_3x3_2_bn.running_var\n",
      "module.base_model.inception_3b_double_3x3_2_bn.num_batches_tracked→eco_2d.inception_b.inception_3b_double_3x3_2_bn.num_batches_tracked\n",
      "module.base_model.inception_3b_pool_proj.weight→eco_2d.inception_b.inception_3b_pool_proj.weight\n",
      "module.base_model.inception_3b_pool_proj.bias→eco_2d.inception_b.inception_3b_pool_proj.bias\n",
      "module.base_model.inception_3b_pool_proj_bn.weight→eco_2d.inception_b.inception_3b_pool_proj_bn.weight\n",
      "module.base_model.inception_3b_pool_proj_bn.bias→eco_2d.inception_b.inception_3b_pool_proj_bn.bias\n",
      "module.base_model.inception_3b_pool_proj_bn.running_mean→eco_2d.inception_b.inception_3b_pool_proj_bn.running_mean\n",
      "module.base_model.inception_3b_pool_proj_bn.running_var→eco_2d.inception_b.inception_3b_pool_proj_bn.running_var\n",
      "module.base_model.inception_3b_pool_proj_bn.num_batches_tracked→eco_2d.inception_b.inception_3b_pool_proj_bn.num_batches_tracked\n",
      "module.base_model.inception_3c_double_3x3_reduce.weight→eco_2d.inception_c.inception_3c_double_3x3_reduce.weight\n",
      "module.base_model.inception_3c_double_3x3_reduce.bias→eco_2d.inception_c.inception_3c_double_3x3_reduce.bias\n",
      "module.base_model.inception_3c_double_3x3_reduce_bn.weight→eco_2d.inception_c.inception_3c_double_3x3_reduce_bn.weight\n",
      "module.base_model.inception_3c_double_3x3_reduce_bn.bias→eco_2d.inception_c.inception_3c_double_3x3_reduce_bn.bias\n",
      "module.base_model.inception_3c_double_3x3_reduce_bn.running_mean→eco_2d.inception_c.inception_3c_double_3x3_reduce_bn.running_mean\n",
      "module.base_model.inception_3c_double_3x3_reduce_bn.running_var→eco_2d.inception_c.inception_3c_double_3x3_reduce_bn.running_var\n",
      "module.base_model.inception_3c_double_3x3_reduce_bn.num_batches_tracked→eco_2d.inception_c.inception_3c_double_3x3_reduce_bn.num_batches_tracked\n",
      "module.base_model.inception_3c_double_3x3_1.weight→eco_2d.inception_c.inception_3c_double_3x3_1.weight\n",
      "module.base_model.inception_3c_double_3x3_1.bias→eco_2d.inception_c.inception_3c_double_3x3_1.bias\n",
      "module.base_model.inception_3c_double_3x3_1_bn.weight→eco_2d.inception_c.inception_3c_double_3x3_1_bn.weight\n",
      "module.base_model.inception_3c_double_3x3_1_bn.bias→eco_2d.inception_c.inception_3c_double_3x3_1_bn.bias\n",
      "module.base_model.inception_3c_double_3x3_1_bn.running_mean→eco_2d.inception_c.inception_3c_double_3x3_1_bn.running_mean\n",
      "module.base_model.inception_3c_double_3x3_1_bn.running_var→eco_2d.inception_c.inception_3c_double_3x3_1_bn.running_var\n",
      "module.base_model.inception_3c_double_3x3_1_bn.num_batches_tracked→eco_2d.inception_c.inception_3c_double_3x3_1_bn.num_batches_tracked\n",
      "module.base_model.res3a_2.weight→eco_3d.res_3d_3.res3a_2.weight\n",
      "module.base_model.res3a_2.bias→eco_3d.res_3d_3.res3a_2.bias\n",
      "module.base_model.res3a_bn.weight→eco_3d.res_3d_3.res3a_bn.weight\n",
      "module.base_model.res3a_bn.bias→eco_3d.res_3d_3.res3a_bn.bias\n",
      "module.base_model.res3a_bn.running_mean→eco_3d.res_3d_3.res3a_bn.running_mean\n",
      "module.base_model.res3a_bn.running_var→eco_3d.res_3d_3.res3a_bn.running_var\n",
      "module.base_model.res3a_bn.num_batches_tracked→eco_3d.res_3d_3.res3a_bn.num_batches_tracked\n",
      "module.base_model.res3b_1.weight→eco_3d.res_3d_3.res3b_1.weight\n",
      "module.base_model.res3b_1.bias→eco_3d.res_3d_3.res3b_1.bias\n",
      "module.base_model.res3b_1_bn.weight→eco_3d.res_3d_3.res3b_1_bn.weight\n",
      "module.base_model.res3b_1_bn.bias→eco_3d.res_3d_3.res3b_1_bn.bias\n",
      "module.base_model.res3b_1_bn.running_mean→eco_3d.res_3d_3.res3b_1_bn.running_mean\n",
      "module.base_model.res3b_1_bn.running_var→eco_3d.res_3d_3.res3b_1_bn.running_var\n",
      "module.base_model.res3b_1_bn.num_batches_tracked→eco_3d.res_3d_3.res3b_1_bn.num_batches_tracked\n",
      "module.base_model.res3b_2.weight→eco_3d.res_3d_3.res3b_2.weight\n",
      "module.base_model.res3b_2.bias→eco_3d.res_3d_3.res3b_2.bias\n",
      "module.base_model.res3b_bn.weight→eco_3d.res_3d_3.res3b_bn.weight\n",
      "module.base_model.res3b_bn.bias→eco_3d.res_3d_3.res3b_bn.bias\n",
      "module.base_model.res3b_bn.running_mean→eco_3d.res_3d_3.res3b_bn.running_mean\n",
      "module.base_model.res3b_bn.running_var→eco_3d.res_3d_3.res3b_bn.running_var\n",
      "module.base_model.res3b_bn.num_batches_tracked→eco_3d.res_3d_3.res3b_bn.num_batches_tracked\n",
      "module.base_model.res4a_1.weight→eco_3d.res_3d_4.res4a_1.weight\n",
      "module.base_model.res4a_1.bias→eco_3d.res_3d_4.res4a_1.bias\n",
      "module.base_model.res4a_1_bn.weight→eco_3d.res_3d_4.res4a_1_bn.weight\n",
      "module.base_model.res4a_1_bn.bias→eco_3d.res_3d_4.res4a_1_bn.bias\n",
      "module.base_model.res4a_1_bn.running_mean→eco_3d.res_3d_4.res4a_1_bn.running_mean\n",
      "module.base_model.res4a_1_bn.running_var→eco_3d.res_3d_4.res4a_1_bn.running_var\n",
      "module.base_model.res4a_1_bn.num_batches_tracked→eco_3d.res_3d_4.res4a_1_bn.num_batches_tracked\n",
      "module.base_model.res4a_2.weight→eco_3d.res_3d_4.res4a_2.weight\n",
      "module.base_model.res4a_2.bias→eco_3d.res_3d_4.res4a_2.bias\n",
      "module.base_model.res4a_down.weight→eco_3d.res_3d_4.res4a_down.weight\n",
      "module.base_model.res4a_down.bias→eco_3d.res_3d_4.res4a_down.bias\n",
      "module.base_model.res4a_bn.weight→eco_3d.res_3d_4.res4a_bn.weight\n",
      "module.base_model.res4a_bn.bias→eco_3d.res_3d_4.res4a_bn.bias\n",
      "module.base_model.res4a_bn.running_mean→eco_3d.res_3d_4.res4a_bn.running_mean\n",
      "module.base_model.res4a_bn.running_var→eco_3d.res_3d_4.res4a_bn.running_var\n",
      "module.base_model.res4a_bn.num_batches_tracked→eco_3d.res_3d_4.res4a_bn.num_batches_tracked\n",
      "module.base_model.res4b_1.weight→eco_3d.res_3d_4.res4b_1.weight\n",
      "module.base_model.res4b_1.bias→eco_3d.res_3d_4.res4b_1.bias\n",
      "module.base_model.res4b_1_bn.weight→eco_3d.res_3d_4.res4b_1_bn.weight\n",
      "module.base_model.res4b_1_bn.bias→eco_3d.res_3d_4.res4b_1_bn.bias\n",
      "module.base_model.res4b_1_bn.running_mean→eco_3d.res_3d_4.res4b_1_bn.running_mean\n",
      "module.base_model.res4b_1_bn.running_var→eco_3d.res_3d_4.res4b_1_bn.running_var\n",
      "module.base_model.res4b_1_bn.num_batches_tracked→eco_3d.res_3d_4.res4b_1_bn.num_batches_tracked\n",
      "module.base_model.res4b_2.weight→eco_3d.res_3d_4.res4b_2.weight\n",
      "module.base_model.res4b_2.bias→eco_3d.res_3d_4.res4b_2.bias\n",
      "module.base_model.res4b_bn.weight→eco_3d.res_3d_4.res4b_bn.weight\n",
      "module.base_model.res4b_bn.bias→eco_3d.res_3d_4.res4b_bn.bias\n",
      "module.base_model.res4b_bn.running_mean→eco_3d.res_3d_4.res4b_bn.running_mean\n",
      "module.base_model.res4b_bn.running_var→eco_3d.res_3d_4.res4b_bn.running_var\n",
      "module.base_model.res4b_bn.num_batches_tracked→eco_3d.res_3d_4.res4b_bn.num_batches_tracked\n",
      "module.base_model.res5a_1.weight→eco_3d.res_3d_5.res5a_1.weight\n",
      "module.base_model.res5a_1.bias→eco_3d.res_3d_5.res5a_1.bias\n",
      "module.base_model.res5a_1_bn.weight→eco_3d.res_3d_5.res5a_1_bn.weight\n",
      "module.base_model.res5a_1_bn.bias→eco_3d.res_3d_5.res5a_1_bn.bias\n",
      "module.base_model.res5a_1_bn.running_mean→eco_3d.res_3d_5.res5a_1_bn.running_mean\n",
      "module.base_model.res5a_1_bn.running_var→eco_3d.res_3d_5.res5a_1_bn.running_var\n",
      "module.base_model.res5a_1_bn.num_batches_tracked→eco_3d.res_3d_5.res5a_1_bn.num_batches_tracked\n",
      "module.base_model.res5a_2.weight→eco_3d.res_3d_5.res5a_2.weight\n",
      "module.base_model.res5a_2.bias→eco_3d.res_3d_5.res5a_2.bias\n",
      "module.base_model.res5a_down.weight→eco_3d.res_3d_5.res5a_down.weight\n",
      "module.base_model.res5a_down.bias→eco_3d.res_3d_5.res5a_down.bias\n",
      "module.base_model.res5a_bn.weight→eco_3d.res_3d_5.res5a_bn.weight\n",
      "module.base_model.res5a_bn.bias→eco_3d.res_3d_5.res5a_bn.bias\n",
      "module.base_model.res5a_bn.running_mean→eco_3d.res_3d_5.res5a_bn.running_mean\n",
      "module.base_model.res5a_bn.running_var→eco_3d.res_3d_5.res5a_bn.running_var\n",
      "module.base_model.res5a_bn.num_batches_tracked→eco_3d.res_3d_5.res5a_bn.num_batches_tracked\n",
      "module.base_model.res5b_1.weight→eco_3d.res_3d_5.res5b_1.weight\n",
      "module.base_model.res5b_1.bias→eco_3d.res_3d_5.res5b_1.bias\n",
      "module.base_model.res5b_1_bn.weight→eco_3d.res_3d_5.res5b_1_bn.weight\n",
      "module.base_model.res5b_1_bn.bias→eco_3d.res_3d_5.res5b_1_bn.bias\n",
      "module.base_model.res5b_1_bn.running_mean→eco_3d.res_3d_5.res5b_1_bn.running_mean\n",
      "module.base_model.res5b_1_bn.running_var→eco_3d.res_3d_5.res5b_1_bn.running_var\n",
      "module.base_model.res5b_1_bn.num_batches_tracked→eco_3d.res_3d_5.res5b_1_bn.num_batches_tracked\n",
      "module.base_model.res5b_2.weight→eco_3d.res_3d_5.res5b_2.weight\n",
      "module.base_model.res5b_2.bias→eco_3d.res_3d_5.res5b_2.bias\n",
      "module.base_model.res5b_bn.weight→eco_3d.res_3d_5.res5b_bn.weight\n",
      "module.base_model.res5b_bn.bias→eco_3d.res_3d_5.res5b_bn.bias\n",
      "module.base_model.res5b_bn.running_mean→eco_3d.res_3d_5.res5b_bn.running_mean\n",
      "module.base_model.res5b_bn.running_var→eco_3d.res_3d_5.res5b_bn.running_var\n",
      "module.base_model.res5b_bn.num_batches_tracked→eco_3d.res_3d_5.res5b_bn.num_batches_tracked\n",
      "module.new_fc.weight→fc_final.weight\n",
      "module.new_fc.bias→fc_final.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習済みモデルをロードする関数の定義\n",
    "\n",
    "def load_pretrained_ECO(model_dict, pretrained_model_dict):\n",
    "    '''ECOの学習済みモデルをロードする関数\n",
    "    今回構築したECOは学習済みモデルとレイヤーの順番は同じだが名前が異なる\n",
    "    '''\n",
    "\n",
    "    # 現在のネットワークモデルのパラメータ名\n",
    "    param_names = []  # パラメータの名前を格納していく\n",
    "    for name, param in model_dict.items():\n",
    "        param_names.append(name)\n",
    "\n",
    "    # 現在のネットワークの情報をコピーして新たなstate_dictを作成\n",
    "    new_state_dict = model_dict.copy()\n",
    "\n",
    "    # 新たなstate_dictに学習済みの値を代入\n",
    "    print(\"学習済みのパラメータをロードします\")\n",
    "    for index, (key_name, value) in enumerate(pretrained_model_dict.items()):\n",
    "        name = param_names[index]  # 現在のネットワークでのパラメータ名を取得\n",
    "        new_state_dict[name] = value  # 値を入れる\n",
    "\n",
    "        # 何から何にロードされたのかを表示\n",
    "        print(str(key_name)+\"→\"+str(name))\n",
    "\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "# 学習済みモデルをロード\n",
    "net_model_ECO = \"./weights/ECO_Lite_rgb_model_Kinetics.pth.tar\"\n",
    "pretrained_model = torch.load(net_model_ECO, map_location='cpu')\n",
    "pretrained_model_dict = pretrained_model['state_dict']\n",
    "# （注釈）\n",
    "# pthがtarで圧縮されているのは、state_dict以外の情報も一緒に保存されているため。\n",
    "# そのため読み込むときは辞書型変数になっているので['state_dict']で指定する。\n",
    "\n",
    "# 現在のモデルの変数名などを取得\n",
    "model_dict = net.state_dict()\n",
    "\n",
    "# 学習済みモデルのstate_dictを取得\n",
    "new_state_dict = load_pretrained_ECO(model_dict, pretrained_model_dict)\n",
    "\n",
    "# 学習済みモデルのパラメータを代入\n",
    "net.eval()\n",
    "net.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 400])\n"
     ]
    }
   ],
   "source": [
    "# 推論\n",
    "net.eval()  \n",
    "\n",
    "batch_iterator = iter(val_dataloader)  # イテレータに変換\n",
    "imgs_transformeds, labels, label_ids, dir_path = next(\n",
    "    batch_iterator)  # 1番目の要素を取り出す\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "    outputs = net(imgs_transformeds)  # ECOで推論\n",
    "\n",
    "print(outputs.shape)  # 出力のサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ファイル： ./data/kinetics_videos/arm wrestling/video0520_sumou_2\n",
      "予測第1位：capoeira\n",
      "予測第2位：wrestling\n",
      "予測第3位：playing cricket\n",
      "予測第4位：somersaulting\n",
      "予測第5位：high kick\n"
     ]
    }
   ],
   "source": [
    "# 予測結果の上位5つを表示します\n",
    "def show_eco_inference_result(dir_path, outputs_input, id_label_dict, idx=0):\n",
    "    '''ミニバッチの各データに対して、推論結果の上位を出力する関数を定義'''\n",
    "    print(\"ファイル：\", dir_path[idx])  # ファイル名\n",
    "\n",
    "    outputs = outputs_input.clone()  # コピーを作成\n",
    "\n",
    "    for i in range(5):\n",
    "        '''1位から5位までを表示'''\n",
    "        output = outputs[idx]\n",
    "        _, pred = torch.max(output, dim=0)  # 確率最大値のラベルを予測\n",
    "        class_idx = int(pred.numpy())  # クラスIDを出力\n",
    "        print(\"予測第{}位：{}\".format(i+1, id_label_dict[class_idx]))\n",
    "        outputs[idx][class_idx] = -1000  # 最大値だったものを消す（小さくする）\n",
    "\n",
    "\n",
    "# 予測を実施\n",
    "idx = 0\n",
    "show_eco_inference_result(dir_path, outputs, id_label_dict, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ファイル： ./data/kinetics_videos/bungee jumping/video0520_2\n",
      "予測第1位：bungee jumping\n",
      "予測第2位：mowing lawn\n",
      "予測第3位：climbing ladder\n",
      "予測第4位：driving tractor\n",
      "予測第5位：cleaning gutters\n"
     ]
    }
   ],
   "source": [
    "# 予測を実施\n",
    "idx = 3\n",
    "show_eco_inference_result(dir_path, outputs, id_label_dict, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ファイル： ./data/kinetics_videos/bungee jumping/video0520_3\n",
      "予測第1位：trimming trees\n",
      "予測第2位：bungee jumping\n",
      "予測第3位：abseiling\n",
      "予測第4位：trapezing\n",
      "予測第5位：climbing ladder\n"
     ]
    }
   ],
   "source": [
    "#その他の予測を実施\n",
    "idx = 5\n",
    "show_eco_inference_result(dir_path, outputs, id_label_dict, idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
